{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Clone repo, install dependencies and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbvMlHd_QwMG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone YOLOv5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import glob, os, errno\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "from numpy import savetxt, loadtxt\n",
        "from sklearn import preprocessing\n",
        "from yolov5 import utils\n",
        "\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhc-Wa9GPBcg"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Downloading the data\n",
        "\n",
        "The data that is being used in this project are obtained from the <a href=\"https://bozcani.github.io/auairdataset\">AU-AIR dataset</a>. Since the labeling was done automatically and pretty inaccurate, the labeling had to be redone manually. For this project, we are only interested in humans and if they are wearing something white or other color. </br>\n",
        "The result of this relabeling was uploaded to a local Google Drive, where the next codeblock will gather the data from. Beside that, there will also be the possibility to upload a dataset.yaml to specify where the model can find the dataset later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGntn2Lkk9z2"
      },
      "outputs": [],
      "source": [
        "%cd data\n",
        "\n",
        "os.path.abspath(os.getcwd())\n",
        "\n",
        "!wget -O train_set.zip # Insert Drive Train Dataset Link Here\n",
        "!unzip train_set.zip\n",
        "!wget -O validatie_set.zip # Insert Drive Validation Dataset Link Here\n",
        "!unzip validatie_set.zip\n",
        "!wget -O test_set.zip # Insert Drive Test Dataset Link Here\n",
        "!unzip test_set.zip\n",
        "\n",
        "os.remove(\"train_set.zip\")\n",
        "os.remove(\"validatie_set.zip\")\n",
        "os.remove(\"test_set.zip\")\n",
        "\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dataset.yaml\n",
        "Import the Dataset.yaml file from a local machine. Within this file, the classes are specified that has to be trained on and where the data could be found."
      ],
      "metadata": {
        "id": "2uhnp4tNhirN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUT6h1olaXYS"
      },
      "outputs": [],
      "source": [
        "print('Upload Dataset.yaml')\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raRruY-J5wvA"
      },
      "source": [
        "# 2. Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt3YQ4lYE3BL"
      },
      "outputs": [],
      "source": [
        "labels_human = Path('data/labels/classified_humans')\n",
        "images_human = Path('data/images/classified_humans')\n",
        "\n",
        "labels_rest = Path('data/labels/rest')\n",
        "images_rest = Path('data/images/rest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okq6UgG9EcV9"
      },
      "source": [
        "## Mirroring data\n",
        "Another possibility to improve the results of the model is the create extra data through data flipping. With this data augmentation method, extra data gets created through the method of horizontally flipping the image and annotation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLys3tEIFKcJ"
      },
      "outputs": [],
      "source": [
        "def flip_annotation(filepath, img, name_operation = \"_flipped\", axis=1):\n",
        "  # Handle with flip data\n",
        "  file_data = []\n",
        "  # open file and read the content in a list\n",
        "  with open(os.path.join(filepath + '.txt'), 'r') as myfile:\n",
        "      for line in myfile:\n",
        "          # remove linebreak which is the last character of the string\n",
        "          currentLine = line[:-1]\n",
        "          data = currentLine.split(\" \")\n",
        "          # add item to the list\n",
        "          file_data.append(data)\n",
        "\n",
        "      # Change X_center Fliplr\n",
        "      for i in file_data:\n",
        "          if len(i) == 5:\n",
        "            i[axis] = str(1 - float(i[axis]) - 1 / img.shape[1])[0:8]\n",
        "\n",
        "\n",
        "      # Write back to the file\n",
        "      f = open(os.path.join(filepath + name_operation + '.txt'), 'w')\n",
        "      for i in file_data:\n",
        "          res = \"\"\n",
        "          for j in i:\n",
        "              res += j + \" \"\n",
        "          f.write(res[:-1]) # Save all but ignore from the least \" \"\n",
        "          f.write(\"\\n\")\n",
        "      f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDWOm5VZFXFj"
      },
      "outputs": [],
      "source": [
        "def flip_images(filepath, img, name_operation=\"_flipped\", axis=1):\n",
        "\n",
        "  # Flip the original image numpy horizontally\n",
        "  horz_img = cv2.flip(img, 1)\n",
        "  cv2.imwrite(os.path.join(filepath + name_operation + '.jpg'), horz_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEIIVy6DI-kO"
      },
      "outputs": [],
      "source": [
        "# Flip the human images and corresponding annotations\n",
        "for fil in labels_human.glob(\"[!classes]*.txt\"):\n",
        "    fil = os.path.splitext(fil)[0]\n",
        "    fil = fil.replace(\"labels\", \"images\")\n",
        "    if os.path.isfile(fil + \".jpg\"):\n",
        "      img = cv2.imread(fil + \".jpg\")\n",
        "      flip_images(fil, img)\n",
        "      fil = fil.replace(\"images\", \"labels\")\n",
        "      flip_annotation(fil, img)\n",
        "\n",
        "# Flip the rest images and corresponding annotations\n",
        "for fil in labels_rest.glob(\"[!classes]*.txt\"):\n",
        "    fil = os.path.splitext(fil)[0]\n",
        "    fil = fil.replace(\"labels\", \"images\")\n",
        "    if os.path.isfile(fil + \".jpg\"):\n",
        "      img = cv2.imread(fil + \".jpg\")\n",
        "      flip_images(fil, img)\n",
        "      fil = fil.replace(\"images\", \"labels\")\n",
        "      flip_annotation(fil, img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ9n2dvDYDXw"
      },
      "source": [
        "## Grayscaling (CURRENTLY NOT BEING USED)\n",
        "\n",
        "One of the possibilities to improve the results of the YOLOv5s model is to add grayscaling to the images. Which means that the colors in the picture will be transformed to grayscaled pictures. It's possible that the model will still be able to tell what the difference is between white clothed and other clothed based on the amount of gray, white or black in a picture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4fACzl250DS"
      },
      "outputs": [],
      "source": [
        "# for fil in images_human.glob(\"*.jpg\"):\n",
        "#     image = cv2.imread(fil)\n",
        "#     gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
        "#     cv2.imwrite(fil,gray_image) # write to location with same name\n",
        "\n",
        "# for fil in images_rest.glob(\"*.jpg\"):\n",
        "#     image = cv2.imread(fil)\n",
        "#     gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
        "#     cv2.imwrite(fil,gray_image) # write to location with same name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAHPz-qgWBLk"
      },
      "source": [
        "# 3. Splitting the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw5VUY6YxMaF"
      },
      "outputs": [],
      "source": [
        "# Check if annotation for image is present\n",
        "for fil in images_rest.glob(\"*.jpg\"):\n",
        "  if not os.path.isfile(str(fil).replace(\".jpg\",\".txt\").replace(\"images\", \"labels\")):\n",
        "    os.remove(fil)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Equalizing the amount of data (CURRENTLY NOT BEING USED)\n",
        "The data gets checked on how many appearances of one class is in the dataset. After that, images with the bigger class gets randomly picked and checked if the other class is included in the image. If not, the image gets deleted. This goes on until both classes are close to equal."
      ],
      "metadata": {
        "id": "Ir6WTBPeh6mh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZYlXfvpRDqh"
      },
      "outputs": [],
      "source": [
        "# white_clothed = 0\n",
        "# other_clothed = 0\n",
        "\n",
        "# for f in labels_human.glob(\"[!classes]*.txt\"):\n",
        "#     with open(f) as text:\n",
        "#         inhoud=text.readlines()\n",
        "#     for line in inhoud:\n",
        "#         print(line[0])\n",
        "#         first_char = line[0]\n",
        "#         if first_char == \"0\":\n",
        "#             white_clothed = white_clothed + 1\n",
        "#         elif first_char == \"1\":\n",
        "#             other_clothed = other_clothed + 1\n",
        "\n",
        "# difference_clothing = other_clothed - white_clothed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZysthP4OTP4"
      },
      "outputs": [],
      "source": [
        "# total_amount_of_other_clothes = 0\n",
        "\n",
        "# while total_amount_of_other_clothes < round(difference_clothing*0.60289):\n",
        "\n",
        "#     delete = True\n",
        "#     file_amount_of_other_clothes = 0\n",
        "\n",
        "#     f = random.choice(list(labels_human.glob('[!classes][frame]*.txt')))\n",
        "\n",
        "#     with open(f) as text:\n",
        "#         inhoud=text.readlines()\n",
        "#     for line in inhoud:\n",
        "#         first_char = line[0]\n",
        "#         if first_char == \"0\":\n",
        "#             delete = False\n",
        "#             continue\n",
        "#         if first_char == \"1\":\n",
        "#             file_amount_of_other_clothes = file_amount_of_other_clothes + 1\n",
        "            \n",
        "#     if delete:\n",
        "#         os.remove(f)\n",
        "#         os.remove(str(f).replace('.txt', '.jpg').replace('labels', 'images'))\n",
        "#         total_amount_of_other_clothes = total_amount_of_other_clothes + file_amount_of_other_clothes\n",
        "\n",
        "#     print(f'{total_amount_of_other_clothes} out of {round(difference_clothing*0.60289)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting\n",
        "After the data has been downloaded, unzipped and equalized, the data has to be split between different datasets to train and test the model with. There needs to be a train, validation and testset. \n",
        "</br></br>\n",
        "The train set will exist of 80% of the total images of humans and 10% of this train amount are rest images which don't include humans. This is done, because it's being recommended by the YOLO documentation to have 10% of the dataset include pictures which doesn't include the recognized object.</br>\n",
        "The validation set will exist of 10% of the images of humans and 10% of this validation amount are images which don't include humans. <br>\n",
        "For the test set, the same is done as with the validation set."
      ],
      "metadata": {
        "id": "Jr-ODCS2jEab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WI6wi61fWANc"
      },
      "outputs": [],
      "source": [
        "os.makedirs('data/train/labels/', exist_ok=True)\n",
        "os.makedirs('data/train/images/', exist_ok=True)\n",
        "os.makedirs('data/validate/labels/', exist_ok=True)\n",
        "os.makedirs('data/validate/images/', exist_ok=True)\n",
        "os.makedirs('data/test/labels/', exist_ok=True)\n",
        "os.makedirs('data/test/images/', exist_ok=True)\n",
        "\n",
        "amount_of_imgs = len(next(os.walk('data/images/classified_humans'))[2])\n",
        "\n",
        "print(amount_of_imgs)\n",
        "\n",
        "train_amount = int(round(amount_of_imgs * 0.815, 0))-1\n",
        "print(train_amount)\n",
        "val_test_amount = int(round(amount_of_imgs * 0.08, 0))-1\n",
        "print(val_test_amount)\n",
        "\n",
        "try:\n",
        "  os.remove(str(labels_human) + '/classes.txt')\n",
        "  os.remove(str(labels_rest) + '/classes.txt')\n",
        "except:\n",
        "  print(\"Files already removed.\") \n",
        "\n",
        "for i in range(0,train_amount):\n",
        "  file = random.choice(os.listdir(labels_human))\n",
        "  shutil.move(str(labels_human) + \"/\" + file, f\"data/train/labels/{file}\")\n",
        "  shutil.move(str(images_human) + \"/\" + file.replace(file[- 4 :], '.jpg'), f\"data/train/images/{file.replace(file[- 4 :], '.jpg')}\")\n",
        "\n",
        "for i in range(0,int(round((train_amount / 10),0))):\n",
        "  file = random.choice(os.listdir(images_rest))\n",
        "  shutil.move(str(images_rest) + \"/\" + file, f\"data/train/images/{file}\")\n",
        "  shutil.move(str(labels_rest) + \"/\" + file.replace(file[- 4 :], '.txt'), f\"data/train/labels/{file.replace(file[- 4 :], '.txt')}\")\n",
        "\n",
        "for i in range(0,val_test_amount):\n",
        "  file = random.choice(os.listdir(labels_human))\n",
        "  shutil.move(str(labels_human) + \"/\" + file, f\"data/validate/labels/{file}\")\n",
        "  shutil.move(str(images_human) + \"/\" + file.replace(file[- 4 :], '.jpg'), f\"data/validate/images/{file.replace(file[- 4 :], '.jpg')}\")\n",
        "\n",
        "for i in range(0,int(round((val_test_amount / 10),0))):\n",
        "  file = random.choice(os.listdir(images_rest))\n",
        "  shutil.move(str(images_rest) + \"/\" + file, f\"data/validate/images/{file}\")\n",
        "  shutil.move(str(labels_rest) + \"/\" + file.replace(file[- 4 :], '.txt'), f\"data/validate/labels/{file.replace(file[- 4 :], '.txt')}\")\n",
        "\n",
        "for i in range(0,val_test_amount):\n",
        "  file = random.choice(os.listdir(labels_human))\n",
        "  shutil.move(str(labels_human) + \"/\" + file, f\"data/test/labels/{file}\")\n",
        "  shutil.move(str(images_human) + \"/\" + file.replace(file[- 4 :], '.jpg'), f\"data/test/images/{file.replace(file[- 4 :], '.jpg')}\")\n",
        "\n",
        "for i in range(0,int(round((val_test_amount / 10),0))):\n",
        "  file = random.choice(os.listdir(images_rest))\n",
        "  shutil.move(str(images_rest) + \"/\" + file, f\"data/test/images/{file}\")\n",
        "  shutil.move(str(labels_rest) + \"/\" + file.replace(file[- 4 :], '.txt'), f\"data/validate/labels/{file.replace(file[- 4 :], '.txt')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# 4. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png\"/></a></p>\n",
        "During the training fase, the YOLOv5s model will be trained to recognize people with white or other colored clothes, based on the data that has been prepared before.\n",
        "</br>\n",
        "\n",
        "- **Pretrained [Models](https://github.com/ultralytics/yolov5/tree/master/models)** are downloaded\n",
        "automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases)\n",
        "- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AskprBhoZfUS"
      },
      "source": [
        "## Preparing WandB\n",
        "\n",
        "With setting up this library, we can see statistics from the model training process during training as well as after training. These results will be kept forever, even when the Google Colab Environment will reset itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fLAV42oNb7M"
      },
      "outputs": [],
      "source": [
        "%pip install -q wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNV447Kpa764"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "In the following code block, the model will be trained. It will standard be training with a picture scale of 920x920, with a batch of 16 and the small weight of the YOLOv5 model. Beside that it will train 50 epochs. </br>\n",
        "Feel free to variate with the parameters to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NcFxRcFdJ_O"
      },
      "outputs": [],
      "source": [
        "# Train YOLOv5s on custom dataset for 50 epochs\n",
        "!python train.py --img 920 --batch 16 --epochs 50 --data dataset.yml --weights yolov5s.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZlMOXCA4Kn6"
      },
      "source": [
        "# 5. Download results locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpDiutNF4JVJ"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/exp .zip /content/yolov5/runs/train/exp\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/exp.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15glLzbQx5u0"
      },
      "source": [
        "# 6. Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLI1JmHU7B0l"
      },
      "source": [
        "## Weights & Biases Logging\n",
        "\n",
        "[Weights & Biases](https://wandb.ai/site?utm_campaign=repo_yolo_notebook) (W&B) is now integrated with YOLOv5 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration for teams. To enable W&B `pip install wandb`, and then train normally (you will be guided through setup on first use). \n",
        "\n",
        "During training you will see live updates at [https://wandb.ai/home](https://wandb.ai/home?utm_campaign=repo_yolo_notebook), and you can create and share detailed [Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results. For more information see the [YOLOv5 Weights & Biases Tutorial](https://github.com/ultralytics/yolov5/issues/1289). \n",
        "\n",
        "<p align=\"left\"><img width=\"900\" alt=\"Weights & Biases dashboard\" src=\"https://user-images.githubusercontent.com/26833433/135390767-c28b050f-8455-4004-adb0-3b730386e2b2.png\"></p>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Recognize Non-Authorized Humans model",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}